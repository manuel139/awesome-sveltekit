/* eslint-disable no-console */
/* This file parses sites.yml, fetches GH metadata like contributors and star count for each
site, then writes the results to site/src/sites.yml. */

import dotenv from 'dotenv'
import fs from 'fs'
import yaml from 'js-yaml'
import fetch from 'node-fetch'
import { basename } from 'path'
import { performance } from 'perf_hooks'
import { root_dir, title_to_slug } from './index.js'

dotenv.config({ path: `${root_dir}/site/.env` })

const in_path = `${root_dir}/sites.yml`
const out_path = `${root_dir}/site/src/sites.yml`
const update_existing = process.argv[2] === `update-existing`

const sites = yaml.load(fs.readFileSync(in_path))

const old_sites = fs.existsSync(out_path)
  ? yaml.load(fs.readFileSync(out_path))
  : []

const this_file = basename(process.argv[1])
console.log(`Running ${this_file}...`)

const start = performance.now()

const prevIds = old_sites.map((site) => site.id)

let [seenSlugs, skipped_sites] = [new Set(), {}]

if (!process.env.GH_TOKEN) {
  console.error(`GH_TOKEN environment variable is not set.`)
  process.exit(1)
}

const headers = {
  authorization: `token ${process.env.GH_TOKEN}`,
}

async function fetch_check(url) {
  const response = await fetch(url, { headers }).then((res) => res.json())
  if (response.message) throw new Error(response.message)
  return response
}

function normalizeUrl(url) {
  if (!url) return null
  if (url.startsWith(`http`)) return url.replace(`http://`, `https://`)
  return `https://${url}`
}

// Only update site/src/sites.js if a new site was added to sites.yml
// or repo star counts were last fetched more than a month ago.
for (const site of sites) {
  const slug = title_to_slug(site.title)

  if (seenSlugs.has(slug)) throw new Error(`Duplicate slug ${slug}`)
  else seenSlugs.add(slug)

  site.slug = slug

  // add open-source tag for all sites with repo key
  if (site.repo && !site.tags.includes(`open source`)) {
    site.tags.push(`open source`)
  }

  if (!site.repo || (prevIds.includes(site.id) && !update_existing)) {
    skipped_sites[site.id] = slug
    continue
  }

  const repoHandle = site.repo.split(`github.com/`)[1]
  if (repoHandle.split(`/`).length !== 2) {
    console.error(`bad repo handle ${repoHandle}`)
    skipped_sites[site.id] = slug
    continue
  }

  const repo_data = await fetch_check(
    `https://api.github.com/repos/${repoHandle}`
  )
  site.repoStars = repo_data.stargazers_count

  let contributors = await fetch_check(
    `https://api.github.com/repos/${repoHandle}/contributors`
  )

  contributors = contributors.filter((c) => c.contributions > 10).slice(0, 5)

  contributors = await Promise.all(
    contributors.map(({ url }) => fetch(url, { headers }).then((r) => r.json()))
  )

  site.contributors = contributors.map(({ name, location, company, ...c }) => ({
    github: c.login,
    twitter: c.twitter_username,
    url: normalizeUrl(c.blog),
    avatar: c.avatar_url,
    name,
    location,
    company,
  }))
}

const new_sites = sites.map((site) => ({
  ...(old_sites.find((itm) => itm.id === site.id) ?? {}),
  ...site,
}))

const n_skipped = Object.keys(skipped_sites).length

const wall_time = ((performance.now() - start) / 1000).toFixed(2)

const comment = `# auto-generated by ${this_file}\n`
fs.writeFileSync(out_path, comment + yaml.dump(new_sites))

console.log(
  `${this_file} took ${wall_time}s, updated ${sites.length} sites, skipped fetching GH metadata for ${n_skipped} sites\n`
)
